{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional GAN tutorial\n",
    "\n",
    " This tutorial covers how to build a conditional GAN for the MNIST dataset, where you can specify the label of the output image by inputting the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction:\n",
    "Generative Adversarial Network (GAN) is a machine learning method that generates “real looking” data from noise. It uses 2 neural networks, one generator and one discriminator. The generator’s job is to generate the data, and the discriminator’s job is to tell whether a sample is true or fake (created by the generator). \n",
    "\n",
    "The networks are trained alternatively for each batch, first the discriminator and then the generator, so that they both improve together against each other (thus “Adversarial”). When the generator is good enough that the discriminator can’t tell a real from a fake image well, you have a good enough generator.\n",
    "\n",
    "A good example of a GAN application is generating handwritten digits, like those in the well known MNIST dataset. A GAN can be trained to generate images just like those in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The problem: What is the label of the output?\n",
    "If we can generate images of digits, the next question is what digit is generated by the GAN? Traditional GANs generate real looking data, but they don’t specify the class of the data they are generating. \n",
    "\n",
    "On the digit example, it’s like the GAN generates the digit, but you can’t know which digit it was.\n",
    "\n",
    "Conditional GANs come to solve this problem with a simple idea, to include label information in the input of both the generator and discriminator, so that the networks can leverage this information when learning and add more control to the generated images.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's start coding a Conditional GAN for the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data \n",
    "We'll start by loading our data.\n",
    "The MNIST dataset is made of 60,000 development samples and 10,000 testing samples. \n",
    "In this tutorial we'll use only the development dataset. As a good practice, \n",
    "we'll also shuffle it after loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MNIST dataset\n",
    "(X_dev,y_dev),(_,_) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# shuffle data\n",
    "indexes = np.arange(X_dev.shape[0],dtype=int)\n",
    "np.random.shuffle(indexes)\n",
    "X_dev = X_dev[indexes]\n",
    "y_dev = y_dev[indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up some Parameters and Processing the Data\n",
    "We'll set up a few parameters, such as batch size and latent dimension (the size of the noise vector). We'll also declare some variables to hold \n",
    "some information on the dataset, such as size, number of channels and classes.\n",
    "\n",
    "We'll also normalize the images to the range of \\[0,1] and convert the labels to one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training images: (60000, 28, 28, 1)\n",
      "Shape of training labels: (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "# define some parameters for the model\n",
    "batch_size = 128\n",
    "num_channels = 1\n",
    "num_classes = 10 \n",
    "image_size = 28\n",
    "latent_dim = 128 # noise vector size\n",
    "\n",
    "# Scale the pixel values to [0, 1] range, add a channel dimension to\n",
    "# the images, and one-hot encode the labels.\n",
    "all_images = X_dev.astype(\"float32\") / 255.0\n",
    "all_images = np.reshape(all_images, (-1, 28, 28, 1))\n",
    "all_labels = tf.keras.utils.to_categorical(y_dev, 10)\n",
    "\n",
    "\n",
    "# Create tf.data.Dataset.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((all_images, all_labels))\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "\n",
    "# just checking shape of training data\n",
    "print(f\"Shape of training images: {all_images.shape}\")\n",
    "print(f\"Shape of training labels: {all_labels.shape}\")\n",
    "\n",
    "\n",
    "generator_in_channels = latent_dim # Number of input channels to the generator (excluding label)\n",
    "discriminator_in_channels = num_channels # Number of input channels to the discriminator (excluding label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Model:\n",
    "The generator needs to have 2 distinct inputs, one for the noise and one for the labels. These must then be joined and the output must be in the same format as the images, 28 by 28 matrices. To do so, we will use Transpose 2D convolutions with 2x2 strides to upscale the dimensions and leaky Relu activations to better handle the sparsity of the dataset. The generator layer diagram is as follows:\n",
    "\n",
    "![alt text](gen.png \"Title\")\n",
    "\n",
    "\n",
    "The generator model is defined as a function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 6272)         809088      ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 49)           539         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 7, 7, 128)    0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 7, 7, 1)      0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 7, 7, 129)    0           ['reshape_1[0][0]',              \n",
      "                                                                  'reshape[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTransp  (None, 14, 14, 256)  528640     ['concatenate[0][0]']            \n",
      " ose)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2DTran  (None, 28, 28, 128)  524416     ['conv2d_transpose[0][0]']       \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 28, 28, 1)    6273        ['conv2d_transpose_1[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,868,956\n",
      "Trainable params: 1,868,956\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Defining Generator model\n",
    "def generator():\n",
    "\n",
    "    # Label input, so that generator can generate \n",
    "    # image of choosen class\n",
    "    label_input = tf.keras.layers.Input((num_classes))\n",
    "    # changing number of nodes with a dense layer \n",
    "    label_dense = tf.keras.layers.Dense(7*7*1, activation=tf.keras.layers.LeakyReLU(alpha=0.2))(label_input)\n",
    "    # Reshaping into 2D So that we can input this into a convolutional layer and concatenate with the noise\n",
    "    label_reshape = tf.keras.layers.Reshape((7,7,1))(label_dense)\n",
    "    \n",
    "    # Noise input\n",
    "    random_input = tf.keras.layers.Input(generator_in_channels)\n",
    "    # dense layer to adjust size\n",
    "    x1 = tf.keras.layers.Dense(7*7*generator_in_channels, activation=tf.keras.layers.LeakyReLU(alpha=0.2))(random_input)\n",
    "    # Reshaping into 2D So that we can input this into a convolutional layer and concatenate\n",
    "    x2 = tf.keras.layers.Reshape((7,7,generator_in_channels))(x1)\n",
    "\n",
    "    #Combining both Label and Noise inputs using concatenation\n",
    "    x3 = tf.keras.layers.Concatenate()([x2,label_reshape])\n",
    "\n",
    "    # Upsample to 14x14 with transpose convolution\n",
    "    x4 = tf.keras.layers.Conv2DTranspose(256,(4,4),strides=(2,2),activation=tf.keras.layers.LeakyReLU(alpha=0.2),padding=\"same\")(x3)\n",
    "    # Upsample to 28x28 with transpose convolution\n",
    "    x5 = tf.keras.layers.Conv2DTranspose(128,(4,4),strides=(2,2),activation=tf.keras.layers.LeakyReLU(alpha=0.2),padding=\"same\")(x4)\n",
    "    # Collapsing 3rd dimension to 1, so that it's the same shape as image\n",
    "    x6 = tf.keras.layers.Conv2D(1,(7,7),padding=\"same\",activation=\"sigmoid\")(x5)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[random_input,label_input], outputs=x6)\n",
    "    return model\n",
    "\n",
    "# declare generator and print summary for inspection\n",
    "generator_model = generator()\n",
    "print(generator_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Model:\n",
    "The discriminator also needs 2 inputs, one for the image and one for the label. Just like in the generator, those inputs will be concatenated\n",
    "and follow on to become the output of the network.\n",
    "\n",
    "The discriminator uses similar layers to the generator, but instead of a Transpose Convolutional layer, it uses a regular convolutional layer with\n",
    "2x2 strides to downsample the dimensions. The discriminator layer diagram is as follows:\n",
    "\n",
    "\n",
    "![alt text](dis.png \"Title\")\n",
    "\n",
    "\n",
    "The discriminator code is defined as a function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 28, 28, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 196)          2156        ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 14, 14, 64)   640         ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)            (None, 14, 14, 1)    0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 14, 14, 65)   0           ['conv2d_1[0][0]',               \n",
      "                                                                  'reshape_2[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 7, 7, 128)    75008       ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " global_max_pooling2d (GlobalMa  (None, 128)         0           ['conv2d_2[0][0]']               \n",
      " xPooling2D)                                                                                      \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            129         ['global_max_pooling2d[0][0]']   \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 77,933\n",
      "Trainable params: 77,933\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def discriminator():\n",
    "    # Label input, so that discriminator knows the label\n",
    "    # of the image it tries to discriminate\n",
    "    label_input = tf.keras.layers.Input((num_classes))\n",
    "    # changing number of nodes with a dense layer \n",
    "    label_dense = tf.keras.layers.Dense(14*14, activation=tf.keras.layers.LeakyReLU(alpha=0.2))(label_input)\n",
    "    # Reshaping into 2D So that we can input this into a convolutional layer and concatenate to image\n",
    "    label_reshape = tf.keras.layers.Reshape((14,14,1))(label_dense)\n",
    "\n",
    "    # Image input\n",
    "    x1 = tf.keras.layers.Input((image_size,image_size,1))\n",
    "    # stride of 2x2 means downsampling to 14x14\n",
    "    x2 = tf.keras.layers.Conv2D(64, (3, 3), strides=(2, 2),activation=tf.keras.layers.LeakyReLU(alpha=0.2), padding=\"same\")(x1) \n",
    "\n",
    "    # Combining Both label and Image inputs using Concatenation, so that label is added in 3rd dimension of shape\n",
    "    x3 = tf.keras.layers.Concatenate()([x2,label_reshape])\n",
    "    # stride of 2x2 downsampling to 7x7\n",
    "    x4 = tf.keras.layers.Conv2D(128, (3, 3), strides=(2, 2),activation=tf.keras.layers.LeakyReLU(alpha=0.2), padding=\"same\")(x3)\n",
    "    # global max collapses first and second dimensions to 1\n",
    "    globalmaxpool = tf.keras.layers.GlobalMaxPooling2D()(x4)\n",
    "    # last layer with 1 node for deciding if image is real or not\n",
    "    final_layer = tf.keras.layers.Dense(1,activation=\"sigmoid\")(globalmaxpool)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[x1,label_input], outputs=final_layer)\n",
    "    return model\n",
    "\n",
    "# declare discriminator and print summary for inspection\n",
    "discriminator_model = discriminator()\n",
    "print(discriminator_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the GAN and Training Procedure\n",
    "The GAN is formed by joining the generator and the discriminator, but the training must be performed differently than in regular neural networks.\n",
    "For each batch, the discriminator must be trained using real and generated data, then the generator must be trained by forward passing the whole\n",
    "GAN with the input noise and assigning the y_data for the discriminator as 1, meaning that we wish for the discriminator to ouput that all samples\n",
    "were true while they were all false. One important note is that the discriminator does not train its weights during the generator training part.\n",
    "\n",
    "To perform this customized training procedure we used the subclassing API to create a ConditionalGAN class, and we altered the steps of the \".fit()\" method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a conditional GAN class by subclassing the Keras model class\n",
    "# This is done to use the .fit function to perform a custom training procedure\n",
    "class ConditionalGAN(tf.keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(ConditionalGAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.gen_loss_tracker = tf.keras.metrics.Mean(name=\"generator_loss\")\n",
    "        self.disc_loss_tracker = tf.keras.metrics.Mean(name=\"discriminator_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.gen_loss_tracker, self.disc_loss_tracker]\n",
    "\n",
    "    # set optimizers for each part of model\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(ConditionalGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    #train_step function is called whenever you use model.fit()\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data\n",
    "        real_images, one_hot_labels = data\n",
    "\n",
    "        # Sample random points in the latent space \n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Decode the noise (guided by labels) to fake images.\n",
    "        fake_images = self.generator([random_latent_vectors,one_hot_labels])\n",
    "\n",
    "        # Ccombine the real images with the fake images\n",
    "        # do the same for the labels (but these just repeat)\n",
    "        all_images = tf.concat([fake_images,real_images],axis=0)\n",
    "        all_labels = tf.concat([one_hot_labels,one_hot_labels],axis=0)\n",
    "\n",
    "        # Assemble labels discriminating real from fake images.\n",
    "        # This is the labels for the discriminator, regarding if the images\n",
    "        # are fake or no\n",
    "        disc_labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "\n",
    "        # Train the discriminator.\n",
    "        with tf.GradientTape() as tape:\n",
    "            #Forward Pass\n",
    "            predictions = self.discriminator([all_images,all_labels])\n",
    "            d_loss = self.loss_fn(disc_labels, predictions)\n",
    "        # Calculate gradients with respect to every trainable variable\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space.\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size*2, self.latent_dim))\n",
    "\n",
    "        # Assemble labels that say \"all real images\".\n",
    "        misleading_labels = tf.zeros((batch_size*2, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            #Forward Pass\n",
    "            fake_images = self.generator([random_latent_vectors,all_labels])\n",
    "            predictions = self.discriminator([fake_images,all_labels])\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        #Calculate gradients with respect to every trainable variable    \n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        # Monitor loss.\n",
    "        self.gen_loss_tracker.update_state(g_loss)\n",
    "        self.disc_loss_tracker.update_state(d_loss)\n",
    "        return {\n",
    "            \"g_loss\": self.gen_loss_tracker.result(),\n",
    "            \"d_loss\": self.disc_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating GAN and Compiling Model\n",
    "We'll now create the GAN using the declared generator and discriminator.\n",
    "\n",
    "Also, we'll compile the GAN with the RMSprop optimizer and slightly different values for the optimizer and discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rodrigo/anaconda3/lib/python3.8/site-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#Intializing the GAN model\n",
    "cond_gan = ConditionalGAN(\n",
    "    discriminator=discriminator_model, generator=generator_model, latent_dim=latent_dim\n",
    ")\n",
    "\n",
    "#Compiling the GAN model\n",
    "cond_gan.compile(\n",
    "    d_optimizer=tf.keras.optimizers.RMSprop(lr = 0.0008, clipvalue = 1.0, decay = 6e-8),\n",
    "    g_optimizer=tf.keras.optimizers.RMSprop(lr = 0.0004, clipvalue = 1.0, decay = 3e-8),\n",
    "    loss_fn=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "Finally, let's train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rodrigo/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 24s 45ms/step - g_loss: 1.0404 - d_loss: 0.5189\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 21s 45ms/step - g_loss: 1.1340 - d_loss: 0.4976\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 21s 45ms/step - g_loss: 1.1046 - d_loss: 0.5270\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 21s 45ms/step - g_loss: 0.9998 - d_loss: 0.5659\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 21s 45ms/step - g_loss: 0.9607 - d_loss: 0.5808\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 21s 45ms/step - g_loss: 0.9464 - d_loss: 0.5957\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 21s 46ms/step - g_loss: 0.9344 - d_loss: 0.6069\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 21s 46ms/step - g_loss: 0.9183 - d_loss: 0.6089\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 21s 46ms/step - g_loss: 0.9225 - d_loss: 0.6088\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 22s 46ms/step - g_loss: 0.9234 - d_loss: 0.6074\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 22s 46ms/step - g_loss: 0.9204 - d_loss: 0.6111\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 22s 48ms/step - g_loss: 0.9192 - d_loss: 0.6105\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 22s 47ms/step - g_loss: 0.9216 - d_loss: 0.6083\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 22s 47ms/step - g_loss: 0.9283 - d_loss: 0.6069\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 22s 48ms/step - g_loss: 0.9309 - d_loss: 0.6043\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 23s 49ms/step - g_loss: 0.9454 - d_loss: 0.6028\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 23s 48ms/step - g_loss: 0.9459 - d_loss: 0.5998\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 23s 49ms/step - g_loss: 0.9518 - d_loss: 0.5970\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 23s 49ms/step - g_loss: 0.9723 - d_loss: 0.5919\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 23s 48ms/step - g_loss: 0.9737 - d_loss: 0.5919\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe0d07a8730>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "cond_gan.fit(dataset, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Results\n",
    "\n",
    "Now that we have a trained model, let's use it to generate some fake data. We'll get the generator of the trained GAN and input noise and labels to it, and show the resulting images together with the labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Verifying if model is working properly\n",
    "\n",
    "# get generator model only\n",
    "trained_gen = cond_gan.generator\n",
    "\n",
    "# alternative to not train (load provided trained model)\n",
    "#trained_gen = tf.keras.models.load_model(\"trained_gen.h5\")\n",
    "\n",
    "\n",
    "# create random noise for input\n",
    "noise = tf.random.normal(shape=(1, latent_dim*9))\n",
    "noise = tf.reshape(noise, (9, latent_dim))\n",
    "\n",
    "# Create random label list\n",
    "random_labels_list = list(np.random.randint(0,10,size=9))\n",
    "\n",
    "# convert to one hot encoding\n",
    "labels = tf.keras.utils.to_categorical(random_labels_list, num_classes)\n",
    "\n",
    "# Use the generator to generate fake images\n",
    "fake = trained_gen.predict([noise,labels])\n",
    "\n",
    "# plot the images in a 3 by 3 plot, with the labels in the title\n",
    "fig,ax = plt.subplots(3,3)\n",
    "\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "for i in range(0,3):\n",
    "    for j in range(0,3):\n",
    "        ax[i,j].imshow(fake[i*3+j],cmap='gray')\n",
    "        ax[i,j].set_title(f\"label: {random_labels_list[i*3+j]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "#saving the generator for later use\n",
    "tf.keras.models.save_model(trained_gen,\"trained_gen.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The End\n",
    "\n",
    "Now we are able to generate the handwritten digit of our choice by controlling the labels inputted to the generator. \n",
    "\n",
    "You can now try to use this CGAN on different datasets or tweak the model!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3f1fa92065e83bf8b3f9b5096216ce92755ea229f7fd6780b8b3e0f199343520"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
